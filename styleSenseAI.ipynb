{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "TJ7Ho7a9VIP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5H564EesakI",
        "outputId": "bc7e8897-03f9-479c-9f79-0196d4167bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.20)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.9.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: fake_useragent in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.27.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.6)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.5.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch diffusers  transformers pillow matplotlib mediapipe opencv-python opencv-contrib-python gradio  torchvision requests beautifulsoup4 fake_useragent\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y wget unzip xvfb libxi6 libgconf-2-4\n",
        "!apt-get install -y libappindicator1 fonts-liberation\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb || apt-get -fy install\n",
        "!rm google-chrome-stable_current_amd64.deb\n",
        "\n",
        "!pip install selenium requests webdriver-manager\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jct6oXUktTRK",
        "outputId": "b39e93ae-765f-4f52-8214-d545c4538d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.189.91.82)] [Connected \r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:5 https://dl.google.com/linux/chrome/deb stable InRelease [1,825 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:9 https://dl.google.com/linux/chrome/deb stable/main amd64 Packages [1,221 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 3,046 B in 1s (2,136 B/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libxi6 is already the newest version (2:1.8-1build1).\n",
            "libgconf-2-4 is already the newest version (3.2.6-7ubuntu2).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-liberation is already the newest version (1:1.07.4-11).\n",
            "libappindicator1 is already the newest version (12.10.1+20.10.20200706.1-0ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "--2024-12-30 00:48:19--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 142.251.2.91, 142.251.2.93, 142.251.2.136, ...\n",
            "Connecting to dl.google.com (dl.google.com)|142.251.2.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112770956 (108M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 107.55M   165MB/s    in 0.7s    \n",
            "\n",
            "2024-12-30 00:48:20 (165 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [112770956/112770956]\n",
            "\n",
            "(Reading database ... 124606 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (131.0.6778.204-1) over (131.0.6778.204-1) ...\n",
            "Setting up google-chrome-stable (131.0.6778.204-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.27.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.28.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (1.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install groq langchain_community sentence_transformers\n",
        "!pip install llama-index-llms-groq\n",
        "!pip install groq\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_DST3pwZt45C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Scrapping"
      ],
      "metadata": {
        "id": "wXDp4A4AVsX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import shutil\n",
        "import urllib.parse\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Websites to scrape\n",
        "websites = {\n",
        "    'junaidjamshed': 'https://www.junaidjamshed.com/womens/kurti.html?product_list_dir=desc&product_list_order=top_rated',\n",
        "    'khaadi': 'https://pk.khaadi.com/ready-to-wear/essentials/kurta/kurta/?prefn1=filter_categories&prefv1=Kurta&srule=most-popular&start=0&sz=96',\n",
        "}\n",
        "\n",
        "# Keywords to filter images (specific to shirts)\n",
        "keywords = ['shirt', 'kurta', 'kurti']\n",
        "\n",
        "# Folder to save images\n",
        "output_folder = \"scraped_images\"\n",
        "\n",
        "# Clear output folder before scraping\n",
        "if os.path.exists(output_folder):\n",
        "    shutil.rmtree(output_folder)  # Delete the folder and its contents\n",
        "os.makedirs(output_folder, exist_ok=True)  # Recreate the folder\n",
        "\n",
        "# Selenium setup\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.binary_location = \"/usr/bin/google-chrome\"\n",
        "\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "\n",
        "# Function to scrape images from Junaid Jamshed using Selenium\n",
        "def scrape_images_junaidjamshed(site_name, url):\n",
        "    try:\n",
        "        driver.get(url)\n",
        "\n",
        "        # Scroll to load all images\n",
        "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        while True:\n",
        "            # Scroll down incrementally and wait for images to load\n",
        "            driver.execute_script(\"window.scrollBy(0, 1000);\")\n",
        "            time.sleep(2)\n",
        "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "            if new_height == last_height:\n",
        "                break\n",
        "            last_height = new_height\n",
        "\n",
        "        # Wait until images are loaded (adjust based on your page structure)\n",
        "        time.sleep(3)\n",
        "\n",
        "        images = []\n",
        "        img_elements = driver.find_elements(By.TAG_NAME, \"img\")\n",
        "\n",
        "        for img in img_elements:\n",
        "            img_url = img.get_attribute('src') or img.get_attribute('data-src') or img.get_attribute('srcset')\n",
        "            alt_text = img.get_attribute('alt')\n",
        "\n",
        "            # Skip base64 images\n",
        "            if img_url and img_url.startswith('data:image'):\n",
        "                continue\n",
        "\n",
        "            # Filter images by keywords in alt text\n",
        "            if alt_text and any(keyword.lower() in alt_text.lower() for keyword in keywords):\n",
        "                images.append((img_url, alt_text))\n",
        "\n",
        "        # Save top 10 images\n",
        "        save_images(site_name, images)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error scraping {site_name}: {e}\")\n",
        "\n",
        "# Function to scrape images from Khaadi using BeautifulSoup\n",
        "def scrape_images_khaadi(site_name, url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        images = []\n",
        "        img_elements = soup.find_all('img')\n",
        "\n",
        "        seen_urls = set()\n",
        "        for img in img_elements:\n",
        "            img_url = img.get('src') or img.get('data-src')\n",
        "            alt_text = img.get('alt')\n",
        "\n",
        "            # Skip base64 images\n",
        "            if img_url and img_url.startswith('data:image'):\n",
        "                continue\n",
        "\n",
        "            # Handle relative URLs\n",
        "            img_url = urllib.parse.urljoin(url, img_url)\n",
        "\n",
        "            # Skip duplicate URLs\n",
        "            if img_url in seen_urls:\n",
        "                continue\n",
        "            seen_urls.add(img_url)\n",
        "\n",
        "            # Filter images by keywords in alt text\n",
        "            if alt_text and any(keyword.lower() in alt_text.lower() for keyword in keywords):\n",
        "                images.append((img_url, alt_text))\n",
        "\n",
        "        # Save top 10 images\n",
        "        save_images(site_name, images)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error scraping {site_name}: {e}\")\n",
        "\n",
        "# Function to save images and descriptions\n",
        "def save_images(site_name, images):\n",
        "    for i, (img_url, description) in enumerate(images[:10]):\n",
        "        try:\n",
        "            if img_url and img_url.startswith('http'):\n",
        "                img_data = requests.get(img_url, timeout=10).content\n",
        "                if img_data:\n",
        "                    img_name = f\"{site_name}shirt{i + 1}.jpg\"\n",
        "                    img_path = os.path.join(output_folder, img_name)\n",
        "\n",
        "                    # Save image\n",
        "                    with open(img_path, 'wb') as img_file:\n",
        "                        img_file.write(img_data)\n",
        "\n",
        "                    # Save description\n",
        "                    desc_path = os.path.join(output_folder, f\"{site_name}shirt{i + 1}.txt\")\n",
        "                    with open(desc_path, 'w') as desc_file:\n",
        "                        desc_file.write(description)\n",
        "\n",
        "                    logging.info(f\"Saved {img_name} with description: {description}\")\n",
        "                else:\n",
        "                    logging.error(f\"Image {img_url} is empty, skipping...\")\n",
        "            else:\n",
        "                logging.warning(f\"Invalid image URL: {img_url}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save image {img_url}: {e}\")\n",
        "\n",
        "# Load images from the output folder\n",
        "def load_images_from_folder(folder_path):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            images.append(filename)\n",
        "    return images\n",
        "\n",
        "# Main scraping function\n",
        "def scrape_data():\n",
        "    all_images = []\n",
        "    for site_name, url in websites.items():\n",
        "        logging.info(f\"Scraping {site_name} for shirts...\")\n",
        "        if site_name == 'junaidjamshed':\n",
        "            scrape_images_junaidjamshed(site_name, url)\n",
        "        elif site_name == 'khaadi':\n",
        "            scrape_images_khaadi(site_name, url)\n",
        "\n",
        "    # Reload the gallery images after scraping\n",
        "    all_images = load_images_from_folder(output_folder)\n",
        "    return all_images\n",
        "\n",
        "# Start scraping\n",
        "images = scrape_data()\n",
        "logging.info(f\"Scraping complete. Images saved: {images}\")\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "z3RzzOUzxPXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Model"
      ],
      "metadata": {
        "id": "q6ZefWnbWWsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "import torchvision.transforms as transforms\n",
        "from typing import List, Dict\n",
        "from groq import Groq\n",
        "\n",
        "# Set up Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "client = Groq()  # Initialize Groq API client\n",
        "DEFAULT_MODEL = \"llama-3.1-70b-versatile\"\n",
        "\n",
        "# Function to create assistant message format\n",
        "def assistant(content: str):\n",
        "    return {\"role\": \"assistant\", \"content\": content}\n",
        "\n",
        "# Function to create user message format\n",
        "def user(content: str):\n",
        "    return {\"role\": \"user\", \"content\": content}\n",
        "\n",
        "# Function for chat completion with Groq\n",
        "def chat_completion(messages: List[Dict], model=DEFAULT_MODEL, temperature=0.6, top_p=0.9) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Preprocess input images\n",
        "def preprocess_image(image_path, size=(512, 512)):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    resize_transform = transforms.Resize(size)\n",
        "    return resize_transform(image)\n",
        "\n",
        "# Load all images from the 'scraped_images' folder\n",
        "def load_images_from_folder(folder_path):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            image = preprocess_image(image_path)\n",
        "            images.append((image, image_path))  # Store image and path tuple\n",
        "    return images\n",
        "\n",
        "# Load all text descriptions from the 'scraped_images' folder\n",
        "def load_descriptions_from_folder(folder_path):\n",
        "    descriptions = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                description = file.read().strip()\n",
        "                descriptions[filename.replace(\".txt\", \".jpg\")] = description\n",
        "    return descriptions\n",
        "\n",
        "# Path to the folder where the scraped images are saved\n",
        "scraped_images_folder = \"scraped_images\"\n",
        "\n",
        "# Global variables to hold loaded images and descriptions\n",
        "images = []\n",
        "descriptions = {}\n",
        "\n",
        "# Function to save uploaded image and description without re-fetching all data\n",
        "def save_uploaded_data(uploaded_image, description):\n",
        "    global images, descriptions\n",
        "\n",
        "    if uploaded_image is not None:\n",
        "        # Save the image to the 'scraped_images' folder\n",
        "        image_path = os.path.join(scraped_images_folder, os.path.basename(uploaded_image))\n",
        "        try:\n",
        "            os.rename(uploaded_image, image_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error renaming file: {e}\")\n",
        "            return gr.update()\n",
        "\n",
        "        # Save the description if provided\n",
        "        if description:\n",
        "            description_path = os.path.join(scraped_images_folder, os.path.basename(uploaded_image).replace(\".jpg\", \".txt\"))\n",
        "            with open(description_path, \"w\", encoding=\"utf-8\") as file:\n",
        "                file.write(description)\n",
        "\n",
        "        # Preprocess and add the new image to the existing images list (without reloading all images)\n",
        "        new_image = preprocess_image(image_path)\n",
        "        images.append((new_image, image_path))\n",
        "\n",
        "        # Add the new description to the existing descriptions dictionary\n",
        "        descriptions[os.path.basename(image_path)] = description\n",
        "\n",
        "        # Update the gallery with the new image only\n",
        "        return gr.update(value=[img[0] for img in images])  # Only update the gallery with the current images\n",
        "    return gr.update()\n",
        "\n",
        "# Load the Stable Diffusion pipeline\n",
        "def load_pipeline():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    return StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(device)\n",
        "\n",
        "# Sync sliders to ensure they sum to 1\n",
        "def sync_sliders(value):\n",
        "    \"\"\" Ensure alpha1 and alpha2 sliders always sum to 1 \"\"\"\n",
        "    if value < 0:\n",
        "        value = 0\n",
        "    elif value > 1:\n",
        "        value = 1\n",
        "    return 1 - value\n",
        "\n",
        "# Blend and generate image using the Stable Diffusion pipeline\n",
        "def blend_and_generate_image(image_paths, alpha1, alpha2, generated_prompt):\n",
        "    if not image_paths or len(image_paths) < 1:\n",
        "        return None\n",
        "\n",
        "    # Ensure alpha1 and alpha2 are within valid range and sum to 1\n",
        "    alpha1 = max(0, min(1, alpha1))\n",
        "    alpha2 = max(0, min(1, alpha2))\n",
        "    if alpha1 + alpha2 != 1:\n",
        "        alpha2 = 1 - alpha1  # Adjust alpha2 to maintain the sum to 1\n",
        "\n",
        "    # Open images and preprocess them\n",
        "    image1 = Image.open(image_paths[0]).convert(\"RGBA\")\n",
        "\n",
        "    # If we only have one image, we'll use it directly (no blending required)\n",
        "    blended_image = image1\n",
        "\n",
        "    # Generate a new image using the blended image and the prompt\n",
        "    pipe = load_pipeline()\n",
        "    output_image = pipe(\n",
        "        prompt=generated_prompt,\n",
        "        image=blended_image.convert(\"RGB\"),\n",
        "        strength=0.80,  # Influence of the input image\n",
        "        guidance_scale=7.5,  # Balances adherence to the prompt\n",
        "        num_inference_steps=50,\n",
        "        generator=torch.manual_seed(42),  # Fixed seed for reproducibility\n",
        "    ).images[0]\n",
        "\n",
        "    return output_image\n",
        "\n",
        "# Generate a creative prompt based on selected images' descriptions\n",
        "def generate_prompt_from_selected_images(image_paths):\n",
        "    selected_descriptions = []\n",
        "\n",
        "    # Get descriptions for the selected images\n",
        "    for image_path in image_paths:\n",
        "        image_name = os.path.basename(image_path)\n",
        "        description = descriptions.get(image_name, \"\")\n",
        "        if description:\n",
        "            selected_descriptions.append(description)\n",
        "\n",
        "    # Generate creative input for the prompt using chat completion (Groq prompt)\n",
        "    if selected_descriptions:\n",
        "        groq_prompt = chat_completion([user(f'generate a new kurta design prompt based on these descriptions: {\", \".join(selected_descriptions)}')])\n",
        "        return groq_prompt\n",
        "    else:\n",
        "        return \"No descriptions found for the selected images.\"\n",
        "\n",
        "# Define the Gradio app\n",
        "def create_ui():\n",
        "    with gr.Blocks() as app:\n",
        "        # Title/Heading Section\n",
        "        with gr.Row():\n",
        "            gr.Markdown(\"### Select images and generate a new image based on descriptions and blending\")\n",
        "\n",
        "        # Buttons and Inputs Section (Upload and Description)\n",
        "        with gr.Row():\n",
        "            fetch_data_button = gr.Button(\"Fetch New Data\")\n",
        "            upload_image = gr.File(label=\"Upload Image\", type=\"filepath\", file_types=[\".jpg\"])\n",
        "            description_input = gr.Textbox(label=\"Image Description\")\n",
        "            upload_button = gr.Button(\"Upload\")\n",
        "\n",
        "        # Image Gallery Section\n",
        "        with gr.Row():\n",
        "            gallery = gr.Gallery(\n",
        "                label=\"Loaded Images\",\n",
        "                value=[],  # Placeholder for images (you may update this dynamically later)\n",
        "                interactive=True,\n",
        "                columns=4,\n",
        "                height=\"auto\",\n",
        "            )\n",
        "\n",
        "        # Display selected image paths\n",
        "        with gr.Row():\n",
        "            image1_display = gr.Textbox(label=\"First Selected Image Path\", interactive=False)\n",
        "            image2_display = gr.Textbox(label=\"Second Selected Image Path\", interactive=False)\n",
        "\n",
        "        # Display the generated prompt\n",
        "        with gr.Row():\n",
        "            generated_prompt_display = gr.Textbox(label=\"Generated Prompt\", interactive=True, lines=3)\n",
        "\n",
        "        # Alpha sliders for blending\n",
        "        with gr.Row():\n",
        "            alpha1_slider = gr.Slider(0, 1, value=0.5, step=0.01, label=\"Weight of First Image\")\n",
        "            alpha2_slider = gr.Slider(0, 1, value=0.5, step=0.01, label=\"Weight of Second Image\")\n",
        "\n",
        "        # Output for generated image\n",
        "        with gr.Row():\n",
        "            output_generated_image = gr.Image(label=\"Generated Image\", type=\"pil\", interactive=False)\n",
        "\n",
        "        # Handle image selection\n",
        "        selected_images = []\n",
        "\n",
        "        def handle_selection(evt: gr.SelectData):\n",
        "            selected_path = images[evt.index][1]  # Get the path (second element of the tuple)\n",
        "            if len(selected_images) < 2:\n",
        "                selected_images.append(selected_path)\n",
        "            else:\n",
        "                selected_images.pop(0)\n",
        "                selected_images.append(selected_path)\n",
        "\n",
        "            # Generate and update the prompt based on selected images\n",
        "            if len(selected_images) > 0:\n",
        "                generated_prompt = generate_prompt_from_selected_images(selected_images)\n",
        "                return selected_images[0] if len(selected_images) > 0 else \"\", selected_images[1] if len(selected_images) > 1 else \"\", generated_prompt\n",
        "            return \"\", \"\", \"\"\n",
        "\n",
        "        gallery.select(handle_selection, None, [image1_display, image2_display, generated_prompt_display])\n",
        "\n",
        "        # Update sliders dynamically and ensure they sum to 1\n",
        "        alpha1_slider.change(sync_sliders, inputs=alpha1_slider, outputs=alpha2_slider)\n",
        "        alpha2_slider.change(sync_sliders, inputs=alpha2_slider, outputs=alpha1_slider)\n",
        "\n",
        "        # Fetch new data\n",
        "        def fetch_new_data_and_update_gallery():\n",
        "            global images, descriptions\n",
        "            updated_images = load_images_from_folder(scraped_images_folder)\n",
        "            descriptions = load_descriptions_from_folder(scraped_images_folder)  # Ensure descriptions are loaded\n",
        "            images = updated_images  # Update the global images list\n",
        "            return gr.update(value=[img[0] for img in updated_images])\n",
        "\n",
        "        fetch_data_button.click(fetch_new_data_and_update_gallery, None, gallery)\n",
        "\n",
        "        # Handle uploading image and description\n",
        "        upload_button.click(save_uploaded_data, [upload_image, description_input], gallery)\n",
        "\n",
        "        # Generate blended image\n",
        "        def blend_and_generate(image1_path, image2_path, alpha1, alpha2, generated_prompt):\n",
        "            return blend_and_generate_image(\n",
        "                [image1_path, image2_path],  # Pass image paths instead of tuples\n",
        "                alpha1,\n",
        "                alpha2,\n",
        "                generated_prompt\n",
        "            )\n",
        "\n",
        "        generate_button = gr.Button(\"Generate Image\")\n",
        "        generate_button.click(\n",
        "            blend_and_generate,\n",
        "            [image1_display, image2_display, alpha1_slider, alpha2_slider, generated_prompt_display],  # Include the prompt here\n",
        "            output_generated_image\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# Launch the Gradio app\n",
        "if __name__ == \"__main__\":\n",
        "    app = create_ui()\n",
        "    app.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POm4hkQU-1fy",
        "outputId": "0446a485-4307-4de3-af2b-1123ded8906e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://aa4475837d61ab6b52.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://aa4475837d61ab6b52.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}